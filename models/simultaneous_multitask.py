import torch.nn as nn

from transformers import BertModel, BertPreTrainedModel, RobertaModel, RobertaPreTrainedModel

from configs import const


class SimultaneousMultitask(BertPreTrainedModel):
    def __init__(self, config, **kwargs):
        """
        Our Multi model
        @param config: a BERTConfig, as used by huggingface
        @param kwargs: kwargs as generated by utils/model_utils.py
        """
        super().__init__(config)

        # should be a list of 2
        self.num_labels = kwargs["num_output_labels"]

        # should be a list of 2
        self.is_multilabel = kwargs["is_multilabel"]

        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

        self.stress_clf = nn.Linear(config.hidden_size, self.num_labels[0])
        self.emotion_clf = nn.Linear(config.hidden_size, self.num_labels[1])

        self.init_weights()

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):
        """
        essentially a copy-paste of the existing .from_pretrained() (e.g., BertForSequenceClassification)
        @param pretrained_model_name_or_path: used by huggingface to identify models
        @param model_args: huggingface model args
        @param kwargs: huggingface model kwargs
        @return: an initialized model using some pretrained checkpoint
        """
        model = super().from_pretrained(pretrained_model_name_or_path, **kwargs)

        return model

    def forward(self, x, type_ids, attn_mask):
        """
        Forward pass of network
        @param x: tensor, (batch_size, seq_length)
        @param type_ids: tensor, (batch_size, seq_length)
        @param attn_mask: tensor, (batch_size, seq_length)
        @return: stress_logits, emo_logits, predictions for each task
        """
        outputs = self.bert(x, token_type_ids=type_ids, attention_mask=attn_mask)

        pooled_output = outputs[1]

        pooled_output = self.dropout(pooled_output)

        stress_logits = self.stress_clf(pooled_output)
        emo_logits = self.emotion_clf(pooled_output)

        return stress_logits, emo_logits

    def get_loss(self, dataset_id, x, type_ids, attn_mask, loss_calc, golds):
        """
        Calculate the loss on one batch with multiple tasks
        @param dataset_id: for compatibility; ignored, we want all the IDs
        @param x: tensor, (batch_size, seq_length)
        @param type_ids: tensor, (batch_size, seq_length)
        @param attn_mask: tensor, (batch_size, seq_length)
        @param loss_calc: an instance of our loss calculator (utils/loss_utils.py)
        @param golds: gold labels, list of [tensor (batch_size, seq_length)]
        @return: (stress_labels, emotion_labels), loss (tensor)
        """
        stress_logits, emo_logits = self(x, type_ids, attn_mask)

        if self.is_multilabel[0]:
            stress_labels = (stress_logits.sigmoid() > const.MULTILABEL_THRESHOLD) * 1
        else:
            stress_labels = stress_logits.argmax(dim=1)

        if self.is_multilabel[1]:
            emotion_labels = (emo_logits.sigmoid() > const.MULTILABEL_THRESHOLD) * 1
        else:
            emotion_labels = emo_logits.argmax(dim=1)

        return (stress_labels, emotion_labels), loss_calc.get_loss((stress_logits, emo_logits), golds)

    def predict(self, dataset_id, x, type_ids, attn_mask):
        """
        Predict labels for one batch--only on the stress task
        @param dataset_id: for compatibility; ignored
        @param x: tensor, (batch_size, seq_length)
        @param type_ids: tensor, (batch_size, seq_length)
        @param attn_mask: tensor, (batch_size, seq_length)
        @return: stress_labels (predicted labels)
        """

        # predict only the stress label for evaluation
        stress_logits, _ = self(x, type_ids, attn_mask)

        if self.is_multilabel[0]:
            stress_labels = (stress_logits.sigmoid() > const.MULTILABEL_THRESHOLD) * 1
        else:
            stress_labels = stress_logits.argmax(dim=1)

        return stress_labels

class RobertaSimultaneousMultitask(RobertaPreTrainedModel):
    def __init__(self, config, **kwargs):
        """
        Our Multi model
        @param config: a BERTConfig, as used by huggingface
        @param kwargs: kwargs as generated by utils/model_utils.py
        """
        super().__init__(config)

        # should be a list of 2
        self.num_labels = kwargs["num_output_labels"]

        # should be a list of 2
        self.is_multilabel = kwargs["is_multilabel"]

        self.roberta = RobertaModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

        self.stress_clf = nn.Linear(config.hidden_size, self.num_labels[0])
        self.emotion_clf = nn.Linear(config.hidden_size, self.num_labels[1])

        self.init_weights()

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):
        """
        essentially a copy-paste of the existing .from_pretrained() (e.g., BertForSequenceClassification)
        @param pretrained_model_name_or_path: used by huggingface to identify models
        @param model_args: huggingface model args
        @param kwargs: huggingface model kwargs
        @return: an initialized model using some pretrained checkpoint
        """
        model = super().from_pretrained(pretrained_model_name_or_path, **kwargs)

        return model

    def forward(self, x, type_ids, attn_mask):
        """
        Forward pass of network
        @param x: tensor, (batch_size, seq_length)
        @param type_ids: tensor, (batch_size, seq_length)
        @param attn_mask: tensor, (batch_size, seq_length)
        @return: stress_logits, emo_logits, predictions for each task
        """
        outputs = self.roberta(x, attention_mask=attn_mask)

        pooled_output = outputs[1]

        pooled_output = self.dropout(pooled_output)

        stress_logits = self.stress_clf(pooled_output)
        emo_logits = self.emotion_clf(pooled_output)

        return stress_logits, emo_logits

    def get_loss(self, dataset_id, x, type_ids, attn_mask, loss_calc, golds):
        """
        Calculate the loss on one batch with multiple tasks
        @param dataset_id: for compatibility; ignored, we want all the IDs
        @param x: tensor, (batch_size, seq_length)
        @param type_ids: tensor, (batch_size, seq_length)
        @param attn_mask: tensor, (batch_size, seq_length)
        @param loss_calc: an instance of our loss calculator (utils/loss_utils.py)
        @param golds: gold labels, list of [tensor (batch_size, seq_length)]
        @return: (stress_labels, emotion_labels), loss (tensor)
        """
        stress_logits, emo_logits = self(x, type_ids, attn_mask)

        if self.is_multilabel[0]:
            stress_labels = (stress_logits.sigmoid() > const.MULTILABEL_THRESHOLD) * 1
        else:
            stress_labels = stress_logits.argmax(dim=1)

        if self.is_multilabel[1]:
            emotion_labels = (emo_logits.sigmoid() > const.MULTILABEL_THRESHOLD) * 1
        else:
            emotion_labels = emo_logits.argmax(dim=1)

        return (stress_labels, emotion_labels), loss_calc.get_loss((stress_logits, emo_logits), golds)

    def predict(self, dataset_id, x, type_ids, attn_mask):
        """
        Predict labels for one batch--only on the stress task
        @param dataset_id: for compatibility; ignored
        @param x: tensor, (batch_size, seq_length)
        @param type_ids: tensor, (batch_size, seq_length)
        @param attn_mask: tensor, (batch_size, seq_length)
        @return: stress_labels (predicted labels)
        """

        # predict only the stress label for evaluation
        stress_logits, _ = self(x, type_ids, attn_mask)

        if self.is_multilabel[0]:
            stress_labels = (stress_logits.sigmoid() > const.MULTILABEL_THRESHOLD) * 1
        else:
            stress_labels = stress_logits.argmax(dim=1)

        return stress_labels